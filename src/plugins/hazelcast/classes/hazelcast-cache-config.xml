<?xml version="1.0" encoding="UTF-8"?>
<!--
  ~ Copyright (c) 2008-2012, Hazel Bilisim Ltd. All Rights Reserved.
  ~
  ~ Licensed under the Apache License, Version 2.0 (the "License");
  ~ you may not use this file except in compliance with the License.
  ~ You may obtain a copy of the License at
  ~
  ~ http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<hazelcast xsi:schemaLocation="http://www.hazelcast.com/schema/config hazelcast-config-2.3.xsd"
           xmlns="http://www.hazelcast.com/schema/config"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
    <group>
        <name>openfire</name>
        <password>openfire</password>
    </group>
    <management-center enabled="false"/>
    <network>
        <port auto-increment="true">5701</port>
        <join>
            <multicast enabled="true">
                <multicast-group>224.2.2.3</multicast-group>
                <multicast-port>54327</multicast-port>
            </multicast>
            <tcp-ip enabled="false"/>
            <aws enabled="false"/>
        </join>
        <interfaces enabled="false"/>
        <ssl enabled="false" />
        <socket-interceptor enabled="false" />
        <symmetric-encryption enabled="false">
            <!--
               encryption algorithm such as
               DES/ECB/PKCS5Padding,
               PBEWithMD5AndDES,
               AES/CBC/PKCS5Padding,
               Blowfish,
               DESede
            -->
            <algorithm>PBEWithMD5AndDES</algorithm>
            <!-- salt value to use when generating the secret key -->
            <salt>thesalt</salt>
            <!-- pass phrase to use when generating the secret key -->
            <password>thepass</password>
            <!-- iteration count to use when generating the secret key -->
            <iteration-count>19</iteration-count>
        </symmetric-encryption>
        <asymmetric-encryption enabled="false">
            <!-- encryption algorithm -->
            <algorithm>RSA/NONE/PKCS1PADDING</algorithm>
            <!-- private key password -->
            <keyPassword>thekeypass</keyPassword>
            <!-- private key alias -->
            <keyAlias>local</keyAlias>
            <!-- key store type -->
            <storeType>JKS</storeType>
            <!-- key store password -->
            <storePassword>thestorepass</storePassword>
            <!-- path to the key store -->
            <storePath>keystore</storePath>
        </asymmetric-encryption>
    </network>
    <partition-group enabled="false"/>
    <executor-service>
        <core-pool-size>16</core-pool-size>
        <max-pool-size>64</max-pool-size>
        <keep-alive-seconds>60</keep-alive-seconds>
    </executor-service>
    <queue name="default">
        <!--
            Maximum size of the queue. When a JVM's local queue size reaches the maximum,
            all put/offer operations will get blocked until the queue size
            of the JVM goes down below the maximum.
            Any integer between 0 and Integer.MAX_VALUE. 0 means
            Integer.MAX_VALUE. Default is 0.
        -->
        <max-size-per-jvm>0</max-size-per-jvm>
        <!--
            Name of the map configuration that will be used for the backing distributed
            map for this queue.
        -->
        <backing-map-ref>default</backing-map-ref>
    </queue>
    <!--
    	Default Openfire cluster configuration. 
    	
    	This defines a distributed map with a local (near cache) component,
    	suitable for stable caches having frequent reads and relatively
		few updates. The cluster-wide limit for items in the map is
		100000, with up to 1000 items available in the local cache. Items 
		in the distributed map will be evicted after an hour of idle time, 
		and items in the local cache(s) will be evicted after 10 minutes
		of idle time.
		
		The cluster will maintain each item in the distributed map on at least
		two nodes (for HA/failover), and any item can be read from the "owner"
		member node or from a backup node.
    -->
    <map name="default">
        <!--
            Number of backups. If 1 is set as the backup-count for example,
            then all entries of the map will be copied to another JVM for
            fail-safety. 0 means no backup.
        -->
        <backup-count>1</backup-count>
        <!--
            Number of async backups. 0 means no backup.
        -->
        <async-backup-count>0</async-backup-count>
        <!--
            Can we read the local backup entries? Default value is false for
            strong consistency. Being able to read backup data will give you
            greater performance.
        -->
        <read-backup-data>true</read-backup-data>
        <!--
			Maximum number of seconds for each entry to stay in the map. Entries that are
			older than <time-to-live-seconds> and not updated for <time-to-live-seconds>
			will get automatically evicted from the map.
			Any integer between 0 and Integer.MAX_VALUE. 0 means infinite. Default is 0.
		-->
        <time-to-live-seconds>0</time-to-live-seconds>
        <!--
			Maximum number of seconds for each entry to stay idle in the map. Entries that are
			idle(not touched) for more than <max-idle-seconds> will get
			automatically evicted from the map. Entry is touched if get, put or containsKey is called.
			Any integer between 0 and Integer.MAX_VALUE. 0 means infinite. Default is 0.
		-->
        <max-idle-seconds>3600</max-idle-seconds>
        <!--
            Valid values are:
            NONE (no eviction),
            LRU (Least Recently Used),
            LFU (Least Frequently Used).
            NONE is the default.
        -->
        <eviction-policy>LRU</eviction-policy>
        <!--
            Maximum size of the map. When max size is reached,
            map is evicted based on the policy defined.
            Any integer between 0 and Integer.MAX_VALUE. 0 means
            Integer.MAX_VALUE. Default is 0.
        -->
        <max-size policy="cluster_wide_map_size">100000</max-size>
        <!--
            When max. size is reached, specified percentage of
            the map will be evicted. Any integer between 0 and 100.
            If 25 is set for example, 25% of the entries will
            get evicted.
        -->
        <eviction-percentage>25</eviction-percentage>
        <!--
            While recovering from split-brain (network partitioning),
            map entries in the small cluster will merge into the bigger cluster
            based on the policy set here. When an entry merge into the
            cluster, there might an existing entry with the same key already.
            Values of these entries might be different for that same key.
            Which value should be set for the key? Conflict is resolved by
            the policy set here. Default policy is hz.ADD_NEW_ENTRY

            There are built-in merge policies such as
            hz.NO_MERGE      ; no entry will merge.
            hz.ADD_NEW_ENTRY ; entry will be added if the merging entry's key
                               doesn't exist in the cluster.
            hz.HIGHER_HITS   ; entry with the higher hits wins.
            hz.LATEST_UPDATE ; entry with the latest update wins.
        -->
        <merge-policy>hz.ADD_NEW_ENTRY</merge-policy>
		<!--
			Near cache provides a local view of the clustered map, which is
			ideal for high-read caches. Each cluster member retains a local
			copy of entries retrieved from the distributed map. This reduces 
			network load for caches that require frequent reads. However, if 
			the	entries are updated frequently, there can be a performance 
			penalty due to invalidations on the other cluster members.
		-->
        <near-cache>
            <max-size>1000</max-size>
            <time-to-live-seconds>0</time-to-live-seconds>
            <max-idle-seconds>600</max-idle-seconds>
            <eviction-policy>LRU</eviction-policy>
            <invalidate-on-change>true</invalidate-on-change>
        </near-cache>
    </map>
    <!-- 
    	Replicated Openfire caches; entries copied to all members (up to 6 max).
     	The settings for these caches were derived from the configuration of 
    	the original clustering plugin.
    -->
    <map name="opt-$cacheStats">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Clearspace SSO Nonce">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
        <time-to-live-seconds>120</time-to-live-seconds>
    </map>
    <map name="Client Session Info Cache">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Javascript Cache">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
        <max-size>256</max-size>
        <time-to-live-seconds>864000</time-to-live-seconds>
        <eviction-policy>LRU</eviction-policy>
    </map>
    <map name="Components Sessions">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Connection Managers Sessions">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Secret Keys Cache">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Validated Domains">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Disco Server Features">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Disco Server Items">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Incoming Server Sessions">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Sessions by Hostname">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Entity Capabilities">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
        <time-to-live-seconds>172800</time-to-live-seconds>
    </map>
    <map name="Routing Servers Cache">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Routing Components Cache">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Routing Users Cache">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Routing Users Sessions">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Routing AnonymousUsers Cache">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <map name="Directed Presences">
        <backup-count>1</backup-count>
        <async-backup-count>5</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    <!-- 
    	Partitioned Openfire caches; entries copied to a single backup node and
    	replicated as needed in each node using near-cache configuration.
     	The settings for these caches were derived from the configuration of 
    	the original clustering plugin.
    -->
    <map name="Roster">
        <backup-count>1</backup-count>
        <read-backup-data>true</read-backup-data>
        <max-idle-seconds>21600</max-idle-seconds>
        <eviction-policy>LRU</eviction-policy>
        <max-size policy="cluster_wide_map_size">100000</max-size>
        <eviction-percentage>10</eviction-percentage>
        <near-cache>
            <max-size>1000</max-size>
            <max-idle-seconds>1800</max-idle-seconds>
            <eviction-policy>LRU</eviction-policy>
            <invalidate-on-change>true</invalidate-on-change>
        </near-cache>
    </map>
    <map name="User">
        <backup-count>1</backup-count>
        <read-backup-data>true</read-backup-data>
        <max-idle-seconds>1800</max-idle-seconds>
        <eviction-policy>LRU</eviction-policy>
        <max-size policy="cluster_wide_map_size">100000</max-size>
        <eviction-percentage>10</eviction-percentage>
        <near-cache>
            <max-size>1000</max-size>
            <max-idle-seconds>300</max-idle-seconds>
            <eviction-policy>LRU</eviction-policy>
            <invalidate-on-change>true</invalidate-on-change>
        </near-cache>
    </map>
    <map name="Group">
        <backup-count>1</backup-count>
        <read-backup-data>true</read-backup-data>
        <max-idle-seconds>1800</max-idle-seconds>
        <eviction-policy>LRU</eviction-policy>
        <max-size policy="cluster_wide_map_size">100000</max-size>
        <eviction-percentage>10</eviction-percentage>
        <near-cache>
            <max-size>1000</max-size>
            <max-idle-seconds>600</max-idle-seconds>
            <eviction-policy>LRU</eviction-policy>
            <invalidate-on-change>true</invalidate-on-change>
        </near-cache>
    </map>
    <map name="Group Metadata Cache">
        <backup-count>1</backup-count>
        <read-backup-data>true</read-backup-data>
        <max-idle-seconds>1800</max-idle-seconds>
    </map>
    <map name="Published Items">
        <backup-count>1</backup-count>
        <read-backup-data>true</read-backup-data>
        <max-size>100000</max-size>
        <time-to-live-seconds>900</time-to-live-seconds>
        <eviction-policy>LRU</eviction-policy>
        <near-cache>
            <max-size>1000</max-size>
            <max-idle-seconds>60</max-idle-seconds>
            <eviction-policy>LRU</eviction-policy>
            <invalidate-on-change>true</invalidate-on-change>
        </near-cache>
    </map>
</hazelcast>